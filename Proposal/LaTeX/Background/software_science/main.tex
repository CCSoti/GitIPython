% give a lot of examples of usages of notebooks in laboratories
% mistakes made then, that will be overcomed with IPython
% the way of writing, connected with IPython again

To clearly understand the problem that this project is trying to solve, we have to go into detail about the concept of using computational applications. The central hypothesis of the research is to analyse over the usage of software in science. However, this area of study wraps great deal of use cases - it can not be measured with general algorithms. A lot of journals and projects that are using coding, would be the evidence and solution of the raised questions - is software helpful for science?; why do we have to use software in analysis?; how a completely different area of study can benefit from the usage of programming scripts? 

\subsection*{Contributions and Approaches}

Software has contributed to a lot of areas of study.

A great deal of researches and journals have been conducted for answering the above questions, which later led to to the idea of observing these investigations in order to assess broadly if software is worth practicing. 

One example for scientifically modeling natural phenomena is the formulation of a snowflake. Norman Packard created the illustration with the use of a program, which demonstrates cellular automation.\cite{wolfram1984computer}\cite{packard1986lattice} The model represents the surface into cells - they have value 0 or 1, which corresponds accordingly to water vapor(black) or to ice(colour). The construction of the model starts with a red cell in the center of the visualisation. Afterwards, it continues by developing series of steps. In order to identify the value of the next cell, the values of the six surrounding cells has to be summed - if it is an odd number, then the cell has to be ice with value 1, if not - the other way around, vapor with value 0. The snowflakes consists of red and blue colours. Its creation requires at least 10,000 calculations. The best possible approach for the accurate and fast generation of the model, was by using computer stimulation. Even if the calculations are simple, the software script helps with the correct build of the pattern.\cite{wolfram1984computer} In figure \ref{fig:snowflake} on page \pageref{fig:snowflake} you can see the snowflake illustration.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{images/snowflake_algorithm}
\caption{Computer Stimulation: Cellular Automation of a snowflake}
\label{fig:snowflake}
\end{figure}

-----------------------------------------------------------------------------------------

Tracking the changes of data in time is crucial for
Animal tracking data helps us understand how individuals and populations move within local areas, migrate across oceans and continents, and evolve through millennia. This information is being used to address environmental challenges such as climate and land use change, biodiversity loss, invasive species, and the spread of infectious diseases.

Despite the obvious power and advantages
of the Argos system to track animals by satellite,
the data generated are difficult for many biologists to
exploit. A broad range of skills is required to efficiently
download, collate, filter and interpret Argos data. Integration
of animal movements with other physical (e.g.
remote sensing imagery) and anthropogenic (e.g. fishery
distributions) datasets presents additional technical
and computing challenges. The Satellite Tracking and
Analysis Tool (STAT) is a freely available system designed
for biologists who work on animal tracking; it
includes a set of standardized tools and techniques for
data management, analysis, and integration with environmental
data.\cite{coyne2005satellite}
-----------------------------------------------------------------------------------------------

Software has an important role in predictions of real life information, such as climate change, automation of systems and artificial intelligence. \cite{easterbrook2009engineering}\cite{chasmSoftware} 

-----------------------------------------------------------------------------------------

Climate scientists build large, complex simulations with
little or no software engineering training, and do not readily
adopt the latest software engineering tools and techniques.
In this paper, we describe an ethnographic study
of the culture and practices of climate scientists at the Met
Office Hadley Centre. The study examined how the scientists
think about software correctness, how they prioritize
requirements, and how they develop a shared understanding
of their models. The findings show that climate scientists
have developed customized techniques for verification
and validation that are tightly integrated into their approach
to scientific research. Their software practices share
many features of both agile and open source projects, in that
they rely on self-organisation of the teams, extensive use of
informal communication channels, and developers who are
also users and domain experts. These comparisons offer
insights into why such practices work.
\textbf{Their approach:} We conducted an eight-week observational study at the
Met Office Hadley Centre, using ethnographic techniques to
identify the concepts and work practices used by the climate
scientists, and to understand their perspectives. We began the study with five key research questions:
• Correctness: How do scientists assess correctness of
their code? What does correctness mean to them?
• Reproducibility: How do scientists ensure experiments
can be reproduced (e.g. for peer review)?
• Shared Understanding: How do scientists develop and
maintain a shared understanding of the large complex
codes they use? E.g. what forms of external representation
do they use when talking about their models?
• Prioritization How do scientists prioritize their requirements?
For example, how do they balance between
doing what is computationally feasible and what
is scientifically interesting?
• Debugging: How do scientists detect (and/or prevent)
errors in the software?
- \textbf{Basics of Climate Modeling} - Climate scientists use a range of different types of computational model in their research. The most sophisticated
are General Circulation Models (GCMs), which represent the atmosphere and oceans using a 3-dimensional grid, and
solve the equations for fluid motion to calculate energy
transfer between grid points. Such models can be run at
different resolutions, depending on the available computing
power.
\textbf{Findings} - The software has a
very long lifetime, is written in an “old” programming language
(Fortran), and performance issues are carefully balanced
with maintainability and portability concerns. The Met Office has evolved a software development process
that is highly adapted to its needs, which relies heavily
on the deep domain knowledge of the scientists building the
software, and is tightly integrated with their scientific research
practices. Our study indicates that for climate science,
at least as practiced at the Met Office, such model validation
is routinely performed, as it is built into a systematic
integration and regression testing process, with each model
run set up as a controlled experiment. Furthermore, climate
science as a field has invested in extensive systems for
collection and calibration of observational data to be used
as a basis for this validation. Climate models have a sound
physical basis and mature, domain-specific software development
processes



% more examples - 1 or 2 maybe.
% that were positive examples for the usage of software in science
% after that, explain the problems - chasm 
% give examples maybe

\subsection*{Methodology and Outcomes}

This section explains how we are assessing the usage of software given all of its history in science.

